---
title: "Simulation of basic causal structures"
author: "Ren√© Valenzuela"
date: "2024-10-28"
date-modified: last-modified
categories: [causal inference]
draft: false
---

```{python}
#| echo: false

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Create the generator
rng = np.random.default_rng()
```

## What is correlation

## What is causation

## Correlation does not imply causation

We consider the following structural causal model (as defined in Section 1.5 of <a href="#cis">(Pearl 2016)</a>)
$$
\begin{align*}
f_{Z} &: Z = 1_{U_{Z} > 0} \\
f_{X} &: X = 1_{Z + U_{X} > 0.5} \\
f_{Y} &: Y = 1_{X + Z + U_{Y} > 2}
\end{align*}
$$
where $U = \{ U_{X}, U_{Y}, U_{Z} \}$ is the set of exogenous variables, $V = \{ X, Y, Z \}$ is the set of endogenous variables and $1_{A}$ is the indicator function of the event $A$. The associated graphical causal model is
```{dot}
digraph G {
  Uz -> Z;
  Ux -> X;
  Z -> X;
  Uy -> Y;
  Z -> Y;
  X -> Y;
}
```

We will assume $U_{i}$ are standard normal random variables. Recall that for binary variables probabilities are equal to means and computing conditional probabilities is particularly easy.

```{python}
n_corr_not_cause = 10000

# Create background factors
u_x = rng.standard_normal(n_corr_not_cause)
u_y = rng.standard_normal(n_corr_not_cause)
u_z = rng.standard_normal(n_corr_not_cause)

# Define variables
z = 1*(u_z > 0)
x = 1*(z + u_x > 0.5)
y = 1*(x + z + u_y > 2)
y_dox = 1*(1 + z + u_y > 2)
```

We see that $P\left( Y \mid \text{do}\left(X\right)\right)$ is different from $P\left( Y \mid X = 1\right)$

```{python}
print(f'{np.mean(y)}, {np.mean(y_dox)}, {np.mean(y[x==1])}')
```



## SEM versus SCM

Consider the following structural equation model,
$$
asp = 2hd
str = asp + hd
$$

which is represented as follows

```{mermaid}
flowchart LR
  hd([HD])
  asp([ASP])
  str([STR])
  hd -- 2 --> asp
  hd -- 1 --> str
  asp -- 1 --> str
```

Our goal is to estimate the coefficients from a sample of the data

```{python}

# Size of sample
n_sample = 100

abt = pd.DataFrame({'hd':rng.random(n_sample)})
abt['asp'] = 2 * abt['hd']
abt['str'] = abt['hd'] + abt['asp']
```

We use a linear regression model to generate the estimation

```{python}
reg = LinearRegression().fit(abt[['asp']], abt[['str']])
```

The coefficient of determination is given by `{python} reg.score(abt[['asp']], abt[['str']])`


```{python}
print(reg.coef_, reg.intercept_)
```

```{python}
reg_adj = LinearRegression().fit(abt[['hd', 'asp']], abt[['str']])
```

```{python}
reg_adj.score(abt[['hd', 'asp']], abt[['str']])
```

```{python}
print(reg_adj.coef_, reg_adj.intercept_)
```

## References

1. Section 1.6 R examples. Causal Data Science with Directed Acyclic graphs. Udemy
2. <a name="cis">Pearl 2016</a>: Causal inference in Statistics. A primer. Judea Pearl, Madelyn Glymour and Nicholas P. Jewell
